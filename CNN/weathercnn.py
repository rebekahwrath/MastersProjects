# -*- coding: utf-8 -*-
"""weatherCNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EeMQ1uizrj6AizbTOots6T8b87HRWIYd
"""

### FROM KAGGLE TO DOWNLOAD DATASET

import kagglehub

# Download latest version
path = kagglehub.dataset_download("jehanbhathena/weather-dataset")

print("Path to dataset files:", path)

import os

# Define the path to the 'dataset' folder - given from kaggle data download
dataset_path = "/root/.cache/kagglehub/datasets/jehanbhathena/weather-dataset/versions/3/dataset"

# List the files and subfolders in the dataset folder - want to know the names of the image folders & the weather classifications
files_in_dataset = os.listdir(dataset_path)
print(files_in_dataset)

### SPOTCHECK THE DATASET AND SEE THE FIRST IMAGE IN EACH OF THE WEATHER CATEGORY FOLDERS

import cv2
import matplotlib.pyplot as plt

# List all subfolders in the dataset directory
subfolders = [f for f in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, f))]

# Create a figure for plotting the images
plt.figure(figsize=(10,10))

# Loop through each subfolder and plot the first image
for i, folder in enumerate(subfolders):
    # full path to the current folder
    folder_path = os.path.join(dataset_path, folder)

    # List of all image files in the folder
    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg'))]

    if image_files:
        # Load the first image from the folder
        image_path = os.path.join(folder_path, image_files[0])
        image = cv2.imread(image_path)

        # Convert BGR to RGB for proper display
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # Plot the image
        plt.subplot(len(subfolders), 2, i + 1)
        plt.imshow(image_rgb)
        plt.title(f"{folder}") # Show the weather classification
        plt.axis('off')

# Show all the images
plt.tight_layout()
plt.show()

### SPLITTING THE DATA INTO TRAINING SET & TESTING SET

import numpy as np
from sklearn.model_selection import train_test_split
import shutil

# Directory where the split data will be stored
output_dir = "/root/.cache/kagglehub/datasets/jehanbhathena/weather-dataset/versions/3/split_data"
train_dir = os.path.join(output_dir, "train") # Training data
test_dir = os.path.join(output_dir, "test") # Testing data

# Create train and test directories
os.makedirs(train_dir, exist_ok=True) # Training directory (where the training data is stored)
os.makedirs(test_dir, exist_ok=True) # Test directory (where the text data is stored)

# List all subfolders in the dataset directory (weather conditions)
subfolders = [f for f in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, f))]

# Lists to hold file paths and corresponding labels
image_paths = []
labels = []

# Loop through each subfolder (weather condition) to gather images
for label, folder in enumerate(subfolders):
    folder_path = os.path.join(dataset_path, folder)
    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg'))]

    # For each image, add the path and label
    for image_file in image_files:
        image_paths.append(os.path.join(folder_path, image_file))
        labels.append(label)

# Split the data into training and testing sets (80% train, 20% test)
train_paths, test_paths, train_labels, test_labels = train_test_split(image_paths, labels, test_size=0.2, stratify=labels, random_state=42)

# Function to copy images to the train/test directories
def copy_images_to_dir(image_paths, labels, target_dir):
    for i, image_path in enumerate(image_paths):
        label = labels[i]
        # Create the label subfolder in the target directory (train or test)
        label_dir = os.path.join(target_dir, subfolders[label])
        os.makedirs(label_dir, exist_ok=True)

        # Copy the image to the appropriate subfolder
        shutil.copy(image_path, label_dir)

# Copy the training and testing images - organizing the images & placing them in the correct directories
copy_images_to_dir(train_paths, train_labels, train_dir)
copy_images_to_dir(test_paths, test_labels, test_dir)

print("Data split completed. Train and test data saved.")

### PREPROCESSING THE DATA TO PREPARE FOR CNN MODEL BUILDING

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define the image size
image_size = (224, 224)

# Initialize ImageDataGenerator for training and testing data
train_datagen = ImageDataGenerator(rescale=1./255,  # Normalize pixel values to [0, 1]
                                   rotation_range=30,
                                   width_shift_range=0.2,
                                   height_shift_range=0.2,
                                   shear_range=0.2,
                                   zoom_range=0.2,
                                   horizontal_flip=True)

test_datagen = ImageDataGenerator(rescale=1./255)  # Only rescale for testing

# Create generators for the training and testing sets
train_generator = train_datagen.flow_from_directory(train_dir,
                                                    target_size=image_size,
                                                    batch_size=32,
                                                    class_mode='sparse',  # For integer labels
                                                    shuffle=True)

test_generator = test_datagen.flow_from_directory(test_dir,
                                                  target_size=image_size,
                                                  batch_size=64, # Increased batch size for faster trianing
                                                  class_mode='sparse',
                                                  shuffle=False)

from tensorflow.keras import layers, models

# Define the CNN model
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)), # First layer with 32 filters, detects simple features
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64, (3, 3), activation='relu'), # Second layer with 64 filters, detects more complex patterns
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(128, (3, 3), activation='relu'), # Third layer with 128 filters, detects the most complex patterns
    layers.MaxPooling2D((2, 2)),

    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(len(subfolders), activation='softmax')  # Output layer for classification
])

model.summary()

import tensorflow as tf
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU'))) # Epochs originally took 3 houts to run, switching runtime to GPU rather than CPU

### Improved compile from part 1 of the assignment updated with my CNN data

model.compile(optimizer='RMSProp',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model.fit(train_generator, epochs=10, validation_data=test_generator)

test_loss, test_acc = model.evaluate(test_generator, verbose=2)

print(f"Test loss: {test_loss}")
print(f"Test accuracy: {test_acc}")

### From professor

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.5, 1])
plt.legend(loc='lower right')

print(test_acc)

from tensorflow.keras.utils import plot_model

plot_model(model, to_file='cnn_weather_model.png', show_shapes=True, show_layer_names=True)

from google.colab import files

# Upload multiple images
uploaded = files.upload()

import matplotlib.pyplot as plt

# Resize and normalize images
for filename in uploaded.keys():
    img = Image.open(io.BytesIO(uploaded[filename]))  # Open image
    img_resized = img.resize(target_size)  # Resize image
    img_array = np.array(img_resized) / 255.0  # Normalize the image
    processed_images.append(img_array)  # Add image to list

# Convert list to numpy array (batch)
image_batch = np.array(processed_images)

# Run the batch through the model
predictions = model.predict(image_batch)

# Get the predicted class labels (weather)
predicted_classes = np.argmax(predictions, axis=1)

# Define class names (weather categories)
class_names = ['dew', 'frost', 'snow', 'hail', 'fogsmog', 'lightning', 'sandstorm', 'rainbow', 'glaze', 'rain', 'rime']

# Display each image and its predicted class
for idx, img in enumerate(processed_images):
    plt.imshow(img)
    plt.title(f"Predicted: {class_names[predicted_classes[idx]]} ({predicted_classes[idx]})")
    plt.axis('off')
    plt.show()