# -*- coding: utf-8 -*-
"""artGAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1agRvff3Qc-yHu8bcaA7KtpDwp2vyNhPL
"""

### FROM KAGGLE, IMPORTING THE DATA

import kagglehub

# Download latest version
path = kagglehub.dataset_download("karnikakapoor/art-portraits")

print("Path to dataset files:", path)

# Importing all the necessary packages for our model

import glob
import imageio
import matplotlib.pyplot as plt
import numpy as np
import os
import PIL
from tensorflow.keras import layers
import time
from PIL import Image

from IPython import display

# Path to the dataset files - from kaggle output
dataset_path = '/root/.cache/kagglehub/datasets/karnikakapoor/art-portraits/versions/3'

# List all files and directories in the dataset path
def list_files_recursive(directory):
    for root, dirs, files in os.walk(directory):
        for name in dirs:
            print(f"Directory: {os.path.join(root, name)}")

# List files and subfolders - list out where all the art portrait images are stored
list_files_recursive(dataset_path)

# List all image files recursively, including files in subfolders - accessing all the art portraits
image_files = []
for root, dirs, files in os.walk(dataset_path):
    for file in files:
        if file.endswith('.jpg'):
            image_files.append(os.path.join(root, file))

# Display the first 5 images so the user can see what the portraits look like
num_images_to_show = 5
plt.figure(figsize=(15, 10))

for i in range(min(num_images_to_show, len(image_files))):
    img_path = image_files[i]

    try:
        img = Image.open(img_path)
        img = img.convert("RGB")

        plt.subplot(1, num_images_to_show, i + 1)
        plt.imshow(np.array(img))
        plt.axis('off')
        plt.title(f"Image {i + 1}")
    except Exception as e:
        print(f"Error loading image {image_files[i]}: {e}")

plt.tight_layout()
plt.show()

from tensorflow.keras.preprocessing import image

# List all image files recursively, including files in subfolders - accessing all the art portraits
image_files = []
for root, dirs, files in os.walk(dataset_path):
    for file in files:
        if file.endswith('.jpg'):
            image_files.append(os.path.join(root, file))

print(f"Found {len(image_files)} image files.")

# Load and preprocess images
def load_images(image_files, image_size=(64, 64)):
    images = []
    for img_path in image_files:
        try:
            # Load and resize image to 64x64 (ensure RGB format)
            img = image.load_img(img_path, target_size=image_size)  # Resize to (64, 64)

            # Convert image to numpy array and ensure it's RGB
            img_array = image.img_to_array(img)

            # Ensure 3 channels (RGB)
            if img_array.shape[-1] != 3:
                img_array = np.repeat(img_array, 3, axis=-1)  # Duplicate channels if the image is grayscale

            # Normalize image to [-1, 1]
            img_array = (img_array / 127.5) - 1  # Normalize to [-1, 1]
            images.append(img_array)
        except Exception as e:
            print(f"Error loading {img_path}: {e}")

    return np.array(images)

# Load all images
art_images = load_images(image_files)
print(f"Loaded {art_images.shape[0]} images with shape {art_images.shape[1:]}.")

# Shuffle and batch the dataset for training
BUFFER_SIZE = 60000
BATCH_SIZE = 64
train_dataset = tf.data.Dataset.from_tensor_slices(art_images)
train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)

# Display the first image in the dataset
import matplotlib.pyplot as plt
plt.imshow(art_images[0])
plt.axis('off')
plt.show()

from tensorflow.keras import layers

def make_generator_model(): # Making the generator model
    model = tf.keras.Sequential()

    # Input: latent vector, first layer
    model.add(layers.Dense(4*4*256, use_bias=False, input_shape=(100,)))
    model.add(layers.BatchNormalization())  # Normalize the output of the dense layer
    model.add(layers.LeakyReLU())  # LeakyReLU activation to allow small negative values

    # Reshape to a 3D feature map (4, 4, 256)
    model.add(layers.Reshape((4, 4, 256)))
    assert model.output_shape == (None, 4, 4, 256)  # (batch_size, 4, 4, 256)

    # First transpose convolution, starting with 128 filters
    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU(alpha=0.2))  # Small leakage to prevent dead neurons

    # Second transpose convolution, 64 filters
    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    # Third transpose convolution, 32 filters
    model.add(layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    # Fourth transpose convolution, ensure the output matches the shape we are expecting (None, 64, 64, 3)
    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))
    assert model.output_shape == (None, 64, 64, 3)  # Final output size

    return model

generator = make_generator_model() # Creating the generator model

noise = tf.random.normal([1, 100])
generated_image = generator(noise, training=False)

plt.imshow(generated_image[0, :, :, 0]) # Creating an image with the untrained generator
plt.show()

def make_discriminator_model(): # Making the discriminator model
    model = tf.keras.Sequential()

    # First convolution, 64 filters, ensuring that it is accepting data that is shape 64, 64, 3
    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[64, 64, 3]))
    model.add(layers.LeakyReLU(alpha=0.2))
    model.add(layers.Dropout(0.3))  # Dropout for regularization, dropping 30%

    # Second convolution, 128 filters
    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))
    model.add(layers.LeakyReLU(alpha=0.2))
    model.add(layers.Dropout(0.3))

    # Third convolution, 256 layers
    model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))
    model.add(layers.LeakyReLU(alpha=0.2))
    model.add(layers.Dropout(0.3))

    # Flatten the output
    model.add(layers.Flatten())

    # Dense layer to output a classification (real or fake)
    model.add(layers.Dense(1))

    return model

discriminator = make_discriminator_model() # Creating the discriminator model
decision = discriminator(generated_image)
print (decision)

cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True) # Defining the binary loss functin (real vs fake aka 0 vs 1)

def discriminator_loss(real_output, fake_output): # Discriminator loss, how well is the discriminator able to tell real vs fake
    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
    total_loss = real_loss + fake_loss
    return total_loss

def generator_loss(fake_output): # Generator loss, how well was the generator able to trick the discriminator
    return cross_entropy(tf.ones_like(fake_output), fake_output)

# Different optimizers as the models are trained separately

generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)
discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)

# Save checkpoint

checkpoint_dir = './training_checkpoints'
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt")
checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,
                                 discriminator_optimizer=discriminator_optimizer,
                                 generator=generator,
                                 discriminator=discriminator)

EPOCHS = 150
noise_dim = 100
num_examples_to_generate = 16

# Progress for animated GIF
seed = tf.random.normal([num_examples_to_generate, noise_dim])

@tf.function
def train_step(art_images):

    noise = tf.random.normal([BATCH_SIZE, noise_dim]) # Generates random noise

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:

      generated_images = generator(noise, training=True) # Generator creates random images using the random noise as an input

      real_output = discriminator(art_images, training=True) # Discriminator classifies real images
      fake_output = discriminator(generated_images, training=True) # Discriminator classifies the generator's images (fake images)

      gen_loss = generator_loss(fake_output) # Generator's loss
      disc_loss = discriminator_loss(real_output, fake_output) # Discriminator's loss

    # Gradients of the generator's and discriminator's loss
    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

    # Update the training variables using the gradients computed
    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))

def train(dataset, epochs):
  for epoch in range(epochs):
    start = time.time()

    for image_batch in dataset:
      train_step(image_batch)

    # Produce images for the GIF
    display.clear_output(wait=True)
    generate_and_save_images(generator,
                             epoch + 1,
                             seed)

    # Save the model every 5 epochs
    if (epoch + 1) % 5 == 0:
      checkpoint.save(file_prefix = checkpoint_prefix)

    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))

  # Generate images after the final epoch
  display.clear_output(wait=True)
  generate_and_save_images(generator,
                           epochs,
                           seed)

def generate_and_save_images(model, epoch, test_input): # Generating and saving the images
  predictions = model(test_input, training=False)

  fig = plt.figure(figsize=(4, 4))

  for i in range(predictions.shape[0]):
      plt.subplot(4, 4, i+1)
      image = predictions[i]
      image = (image + 1) / 2.0  # Rescale
      image = tf.clip_by_value(image, 0.0, 1.0)  # Clip to valid range [0, 1]

      plt.imshow(image)
      plt.axis('off')  # Hide axes for a cleaner view

  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))
  plt.show()

train(train_dataset, EPOCHS) # Train the model

checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))

# Display a single image using the epoch number (From Professor)
def display_image(epoch_no):
  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))

display_image(EPOCHS) # Display the images after the final epoch

# Creating our GIF (From Professor)

anim_file = 'dcgan.gif'

with imageio.get_writer(anim_file, mode='I') as writer:
  filenames = glob.glob('image*.png')
  filenames = sorted(filenames)
  for filename in filenames:
    image = imageio.imread(filename)
    writer.append_data(image)
  image = imageio.imread(filename)
  writer.append_data(image)

!pip install tensorflow-docs # Installing the proper packages

# Final GIF showing improvement through the epochs
import tensorflow_docs.vis.embed as embed
embed.embed_file(anim_file)

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np

# Step 1: Generate random latent vectors
batch_size = 16
latent_dim = 100
random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))

# Step 2: Generate images using the Generator
sample_images = generator(random_latent_vectors, training=False)

# Step 3: Display the generated images
fig, axes = plt.subplots(4, 4, figsize=(10, 10))
for i in range(16):
    ax = axes[i // 4, i % 4]
    ax.imshow(sample_images[i] * 0.5 + 0.5)  # Rescale if necessary
    ax.axis('off')
plt.tight_layout()
plt.show()

# Step 4: Optionally, evaluate the images with the Discriminator
discriminator_output = discriminator(sample_images, training=False)
print("Discriminator Output (real/fake) for generated images:")
print(discriminator_output.numpy())

from tensorflow.keras.utils import plot_model

plot_model(generator, to_file='gan_generator_art_model.png', show_shapes=True, show_layer_names=True)

plot_model(discriminator, to_file='gan_discriminator_art_model.png', show_shapes=True, show_layer_names=True)